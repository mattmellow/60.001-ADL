{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANFingerprint Deepfake Detection\n",
    "\n",
    "This notebook provides a complete walkthrough of training, evaluating, and using the GANFingerprint deepfake detection model. The model is designed to detect GAN-generated fake images by analyzing subtle fingerprint patterns in both spatial and frequency domains.\n",
    "\n",
    "## What are GAN Fingerprints?\n",
    "GAN Fingerprints are distinctive patterns or traces that are unintentionally embedded in images generated by Generative Adversarial Networks (GANs). These GAN fingerprints are akin to real human fingerprints, with the comparison that humans unintentionally leave fingerprints on the items they touch, that can be used to trace their identities. Just like human fingerprints, these GAN Fingerprints are unique to the GAN architecture the images are generated from, due to these factors:\n",
    "\n",
    "1. Each GAN architecture has its own unique way of generating images based on its specific design, loss functions, and optimization methods.\n",
    "\n",
    "2. Even GANs with identical architectures but different training datasets, random initializations, or hyperparameters will produce images with subtly different characteristics.\n",
    "\n",
    "## Objective of the project\n",
    "\n",
    "With GAN image generation images getting more advanced, there may be difficulties identifying deepfake images through existing methods, such as detecting distortions in facial features and image details. Through our project, we hope to create a deepfake detection model that can identify deepfake images reliably, no matter how realistic the generated images are to the human eye. By customizing and creating a model that can discriminate deepfake images from real ones through their GAN Fingerprint profiles, we hope to come up with a more sophisticated model which can capture details invisible to the human eye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "#### First, let's install all necessary dependencies so that the model runs properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Virtual Environment for model\n",
    "!python3 -m venv myenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next: Activate the Virtual Environment (manually in terminal)\n",
    "\n",
    "**Navigate to the deepfake-detection-GANFingerprint folder, making it the root folder in your IDE.**\n",
    "\n",
    "Open a terminal in the notebook's folder, then run:\n",
    "\n",
    "- **On macOS/Linux/WSL**:\n",
    "  ```bash\n",
    "  source myenv/bin/activate\n",
    "  ```\n",
    "\n",
    "- **On Windows**:\n",
    "  ```cmd\n",
    "  myenv\\Scripts\\activate\n",
    "  ```\n",
    "\n",
    "Once activated, your terminal prompt will show `(myenv)`, meaning you're using the virtual environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, install all necessary dependencies using requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# For CUDA 11.8 support (recommended for newer NVIDIA GPUs)\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# For CPU-only installation (if no GPU is available)\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's verify that we're running from the correct directory and all required files are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append('.')\n",
    "\n",
    "# Check if all required files exist\n",
    "files_needed = [\n",
    "    \"config.py\",\n",
    "    \"data_loader.py\",\n",
    "    \"models/__init__.py\",\n",
    "    \"models/fingerprint_net.py\", \n",
    "    \"models/layers.py\",\n",
    "    \"train.py\",\n",
    "    \"evaluate.py\",\n",
    "    \"inference.py\",\n",
    "    \"utils/metrics.py\",\n",
    "    \"utils/experiment.py\",\n",
    "    \"utils/visualization.py\",\n",
    "    \"utils/reproducibility.py\",\n",
    "    \"utils/augmentations.py\",\n",
    "    \"utils/gradcam.py\"\n",
    "]\n",
    "\n",
    "missing = [f for f in files_needed if not os.path.exists(f)]\n",
    "if missing:\n",
    "    print(\"❌ Missing required files:\")\n",
    "    for f in missing:\n",
    "        print(f\"  - {f}\")\n",
    "    print(\"\\nPlease run this notebook from the project root directory\")\n",
    "else:\n",
    "    print(\"✅ All required files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Directory and dataset required\n",
    "\n",
    "If configured properly, the model should have the following directory layout:\n",
    "\n",
    "```\n",
    "deepfake_detector/\n",
    "├── config.py                 # Configuration parameters\n",
    "├── data_loader.py            # Dataset and dataloader implementation\n",
    "├── models/\n",
    "│   ├── __init__.py           # Module initialization\n",
    "│   ├── fingerprint_net.py    # GANFingerprint model architecture\n",
    "│   ├── layers.py             # Custom layers and blocks\n",
    "├── train.py                  # Training script\n",
    "├── evaluate.py               # Evaluation script\n",
    "├── inference.py              # Inference on new images\n",
    "├── utils/\n",
    "│   ├── __init__.py           # Utilities module initialization\n",
    "│   ├── reproducibility.py    # Random seed and reproducibility utilities\n",
    "│   ├── visualization.py      # Plotting and visualization tools\n",
    "│   ├── metrics.py            # Performance metrics calculation\n",
    "│   ├── augmentations.py      # Advanced augmentation techniques\n",
    "|   ├── experiment.py         # Logging of information when training model\n",
    "|   ├── gradcam.py            # Grad-CAM visualization of inference results\n",
    "├── checkpoints/              # Directory for saved model checkpoints\n",
    "├── logs/                     # TensorBoard logs and training records\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Pytorch and CUDA status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch version and CUDA availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "from data_loader import get_dataset_stats\n",
    "from models import FingerprintNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Current Configuration\n",
    "\n",
    "The model allows for configuration of hyperparameters. The hyperparameters that led to the best results are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display current configuration\n",
    "print(\"Current Configuration:\")\n",
    "print(f\"DATA_ROOT: {config.DATA_ROOT}\")\n",
    "print(f\"INPUT_SIZE: {config.INPUT_SIZE}\")\n",
    "print(f\"BACKBONE: {config.BACKBONE}\")\n",
    "print(f\"BATCH_SIZE: {config.BATCH_SIZE}\")\n",
    "print(f\"EARLY_STOPPING_PATIENCE: {config.EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"LEARNING_RATE: {config.LEARNING_RATE}\")\n",
    "print(f\"WEIGHT_DECAY: {config.WEIGHT_DECAY}\")\n",
    "print(f\"NUM_EPOCHS: {config.NUM_EPOCHS}\")\n",
    "print(f\"NUM_WORKERS: {config.NUM_WORKERS}\")\n",
    "print(f\"DROPOUT_RATE: {config.DROPOUT_RATE}\")\n",
    "print(f\"DEVICE: {config.DEVICE}\")\n",
    "print(f\"USE_AMP: {config.USE_AMP}\")\n",
    "print(f\"CHECKPOINT_DIR: {config.CHECKPOINT_DIR}\")\n",
    "print(f\"LOG_DIR: {config.LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Dataset Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dataset we will be using is the  'deepfake and real images' dataset by Manjil Kariki.\n",
    "\n",
    "Link: https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images\n",
    "\n",
    "Download the dataset and place it in the root directory in a folder named 'data'. The cell below will help you check if your directory is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset structure\n",
    "def check_dataset_structure():\n",
    "    paths = [\n",
    "        config.TRAIN_REAL_DIR,\n",
    "        config.TRAIN_FAKE_DIR,\n",
    "        config.VAL_REAL_DIR,\n",
    "        config.VAL_FAKE_DIR,\n",
    "        config.TEST_REAL_DIR,\n",
    "        config.TEST_FAKE_DIR\n",
    "    ]\n",
    "    \n",
    "    for path in paths:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"❌ {path} does not exist!\")\n",
    "        else:\n",
    "            print(f\"✅ {path} exists with {len(os.listdir(path))} images\")\n",
    "\n",
    "check_dataset_structure()\n",
    "get_dataset_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Display Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample images from the dataset\n",
    "def show_samples(real_dir, fake_dir, n=5):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(config.INPUT_SIZE),\n",
    "        transforms.CenterCrop(config.INPUT_SIZE),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(real_dir) or not os.path.exists(fake_dir):\n",
    "        print(f\"Error: One or more directories do not exist:\\n{real_dir}\\n{fake_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Get image lists\n",
    "    real_files = os.listdir(real_dir)\n",
    "    fake_files = os.listdir(fake_dir)\n",
    "    \n",
    "    if not real_files or not fake_files:\n",
    "        print(\"Error: One or more directories are empty\")\n",
    "        return\n",
    "    \n",
    "    real_images = [os.path.join(real_dir, f) for f in real_files[:n]]\n",
    "    fake_images = [os.path.join(fake_dir, f) for f in fake_files[:n]]\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i, img_path in enumerate(real_images + fake_images):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = transform(img)\n",
    "        \n",
    "        plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(img_tensor.permute(1, 2, 0))\n",
    "        plt.title(\"Real\" if i < n else \"Fake\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show samples from training set\n",
    "try:\n",
    "    show_samples(config.TRAIN_REAL_DIR, config.TRAIN_FAKE_DIR)\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying samples: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize the model with overview of trainable paramaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the cell below will show the architecture of the model and the number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = FingerprintNet(backbone=config.BACKBONE)\n",
    "model = model.to(config.DEVICE)\n",
    "\n",
    "# Count model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Print model architecture summary\n",
    "print(model)\n",
    "print(f\"Total trainable parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to train the model.\n",
    "\n",
    "This cell allows you to resume training from a saved checkpoint in the checkpoints directory. To do so:\n",
    "\n",
    "Simply comment ```best_checkpoint = train_model()```\n",
    "\n",
    "and uncomment ```best_checkpoint = train_model(resume_checkpoint=\"checkpoints/ganfingerprint_.pth\")```, inserting the correct relative directory of your last saved checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train as train_function\n",
    "\n",
    "# Wrapper for the training function\n",
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "def train_model(data_root=config.DATA_ROOT, \n",
    "                batch_size=config.BATCH_SIZE,\n",
    "                num_workers=config.NUM_WORKERS, \n",
    "                lr=config.LEARNING_RATE, \n",
    "                weight_decay=config.WEIGHT_DECAY,\n",
    "                dropout_rate=config.DROPOUT_RATE,\n",
    "                epochs=config.NUM_EPOCHS, \n",
    "                early_stopping_patience=config.EARLY_STOPPING_PATIENCE,\n",
    "                backbone=config.BACKBONE,\n",
    "                no_amp=not config.USE_AMP, \n",
    "                resume_checkpoint=None):\n",
    "    \n",
    "    # Override config values if needed\n",
    "    config.DATA_ROOT = data_root\n",
    "    config.BATCH_SIZE = batch_size\n",
    "    config.NUM_WORKERS= num_workers\n",
    "    config.LEARNING_RATE = lr\n",
    "    config.WEIGHT_DECAY = weight_decay\n",
    "    config.DROPOUT_RATE = dropout_rate\n",
    "    config.NUM_EPOCHS = epochs\n",
    "    config.EARLY_STOPPING_PATIENCE = early_stopping_patience\n",
    "    config.BACKBONE = backbone\n",
    "    config.USE_AMP = not no_amp\n",
    "    \n",
    "    # Create args object\n",
    "    args = Args(\n",
    "        data_root=data_root,\n",
    "        batch_size=batch_size,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        backbone=backbone,\n",
    "        no_amp=no_amp,\n",
    "        resume_checkpoint=resume_checkpoint\n",
    "    )\n",
    "    \n",
    "    # Call the training function\n",
    "    train_function(args)\n",
    "    \n",
    "    # Return the path to the best checkpoint\n",
    "    return os.path.join(config.CHECKPOINT_DIR, f\"ganfingerprint_best.pth\")\n",
    "\n",
    "# Train the model \n",
    "best_checkpoint = train_model()\n",
    "\n",
    "# To resume training from a checkpoint (uncomment to run):\n",
    "# best_checkpoint = train_model(resume_checkpoint=\"checkpoints/ganfingerprint_.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluating the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell lets you evaluate the model you just trained, showing you various statistics and visualizations.\n",
    "\n",
    "To do so, replace the input of this function call ```evaluate_model(\"checkpoints\\ganfingerprint_best.pth\")``` with the relative path of your checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate as evaluate_function\n",
    "\n",
    "# Wrapper for the evaluation function\n",
    "def evaluate_model(checkpoint_path, output_dir=\"eval_results\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the model checkpoint\n",
    "        output_dir: Directory to save evaluation results\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Call the evaluation function\n",
    "    evaluate_function(checkpoint_path, output_dir)\n",
    "        \n",
    "    # Display the generated images\n",
    "    image_paths = [\n",
    "        os.path.join(output_dir, \"confusion_matrix.png\"),\n",
    "        os.path.join(output_dir, \"roc_curve.png\"),\n",
    "        os.path.join(output_dir, \"precision_recall_curve.png\")\n",
    "    ]\n",
    "    \n",
    "\n",
    "# Evaluate the model (Insert relative directory to trained model in checkpoints folder to run the evaluation)\n",
    "evaluate_model(\"checkpoints\\ganfingerprint_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Testing the model with actual images\n",
    "\n",
    "After training and evaluating the model, we can try out the capabilities of the model by having it classify any image that is outside of the dataset. Lets see if it is able to classify images correctly!\n",
    "\n",
    "By passing images that are named with their ground truth labels (e.g ```true_image```), the inference functions are able to provide you with various statistics indicating the model performance, such as precision and recall for batch inference. \n",
    "\n",
    "After running a batch inference, the results will be saved under ```inference_results```, a folder created to store the prediction outputs.\n",
    "\n",
    "The inference function features Heatmaps created by ```Grad-CAM```, which will show us parts of the images the model looks out for to determine its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single image inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To use this cell:\n",
    "\n",
    "Replace the placeholders for ```model_checkpoint``` and ```test_image_path``` with the relevant relative directories and then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.serialization\n",
    "import os\n",
    "from inference import run_inference\n",
    "\n",
    "# Add numpy scalar to safe globals for PyTorch 2.6+ compatibility\n",
    "torch.serialization.add_safe_globals(['numpy._core.multiarray.scalar'])\n",
    "\n",
    "# Function to run single image inference\n",
    "def run_single_inference(checkpoint_path, image_path, output_dir=None):\n",
    "    \"\"\"\n",
    "    Run inference on a single image using functions from inference.py\n",
    "    \"\"\"\n",
    "    # Fix path separators if needed\n",
    "    checkpoint_path = checkpoint_path.replace('\\\\', '/')\n",
    "    image_path = image_path.replace('\\\\', '/')\n",
    "    if output_dir:\n",
    "        output_dir = output_dir.replace('\\\\', '/')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run inference\n",
    "    print(f\"Running inference on: {image_path}\")\n",
    "    print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "    run_inference(checkpoint_path, image_path, output_dir, batch_mode=False)\n",
    "\n",
    "model_checkpoint = \"checkpoints\\ganfingerprint_best.pth\"\n",
    "test_image_path =  \"path_to_image.jpg\"\n",
    "\n",
    "run_inference(model_checkpoint, test_image_path, use_gradcam=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To use this cell:\n",
    "\n",
    "Replace the placeholders for ```model_checkpoint``` and ```test_dir``` with the relevant relative directories and then run the cell.\n",
    "\n",
    "Note that instead of a single image, ```test_dir``` should lead to a folder of images you want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import run_inference\n",
    "import os\n",
    "\n",
    "def run_batch_inference(checkpoint_path, image_dir, output_dir=\"inference_results\", use_gradcam=False):\n",
    "    \"\"\"\n",
    "    Run inference on a directory of images using functions from inference.py\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the model checkpoint\n",
    "        image_dir: Directory containing images to process\n",
    "        output_dir: Directory to save results\n",
    "        use_gradcam: Whether to generate Grad-CAM visualizations\n",
    "    \"\"\"\n",
    "    # Fix path separators if needed\n",
    "    checkpoint_path = checkpoint_path.replace('\\\\', '/')\n",
    "    image_dir = image_dir.replace('\\\\', '/')\n",
    "    output_dir = output_dir.replace('\\\\', '/')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run inference\n",
    "    print(f\"Running batch inference on directory: {image_dir}\")\n",
    "    print(f\"Using checkpoint: {checkpoint_path}\")\n",
    "    print(f\"Grad-CAM visualization: {'Enabled' if use_gradcam else 'Disabled'}\")\n",
    "    run_inference(checkpoint_path, image_dir, output_dir, batch_mode=True, use_gradcam=use_gradcam)\n",
    "\n",
    "model_checkpoint = \"checkpoints\\ganfingerprint_best.pth\"\n",
    "test_dir = \"relative/path/to/folder\"\n",
    "\n",
    "# Run inference on a directory of images WITH Grad-CAM visualization\n",
    "run_batch_inference(model_checkpoint, test_dir, \"inference_results_with_gradcam\", use_gradcam=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does this mean that the GANFingerprint model is the best?\n",
    "\n",
    "No. In our case, this GANFingerprint detection model is trained on a dataset that has images generated from pre-GPT GAN models.\n",
    "\n",
    "As a result, this model will fail to classify GAN Generated images from modern Generation Models, like **ChatGPT**.\n",
    "\n",
    "Use this interactive widget with Grad-CAM to see where the model is looking at to make its predictions. Are they looking too much into the background, rather than facial features?\n",
    "\n",
    "The cell below will generate a basic interface that allows you to upload any generated image of dimension 256x256 pixels. Feel free to pit modern GAN Generated images\n",
    "(or any image with a face) against the model you trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced interface for GANFingerprint detector with Grad-CAM visualization\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import io\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Import from existing inference.py\n",
    "from inference import predict_image_calibrated, run_inference\n",
    "import config\n",
    "from models import FingerprintNet\n",
    "\n",
    "# For Grad-CAM\n",
    "from utils.gradcam import get_gradcam_layer, GradCAM, generate_gradcam\n",
    "\n",
    "# Set the checkpoint path - user only needs to modify this line\n",
    "MODEL_CHECKPOINT = \"checkpoints\\ganfingerprint_best.pth\"\n",
    "MODEL_CHECKPOINT = MODEL_CHECKPOINT.replace('\\\\', '/')\n",
    "\n",
    "# Load the model once using existing code\n",
    "print(\"Loading model from\", MODEL_CHECKPOINT)\n",
    "model = FingerprintNet(backbone=config.BACKBONE)\n",
    "checkpoint = torch.load(MODEL_CHECKPOINT, map_location=config.DEVICE, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(config.DEVICE)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Define the image transformation (same as in inference.py)\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config.INPUT_SIZE),\n",
    "    transforms.CenterCrop(config.INPUT_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to extract image data from uploaded file (handles different widget versions)\n",
    "def extract_image_data(uploaded_file):\n",
    "    \"\"\"Extract image data from the uploaded file object\"\"\"\n",
    "    if hasattr(uploaded_file, 'content'):\n",
    "        return uploaded_file.content\n",
    "    elif hasattr(uploaded_file, 'value') and isinstance(uploaded_file.value, bytes):\n",
    "        return uploaded_file.value\n",
    "    elif callable(getattr(uploaded_file, 'getvalue', None)):\n",
    "        return uploaded_file.getvalue()\n",
    "    elif hasattr(uploaded_file, 'data'):\n",
    "        return uploaded_file.data\n",
    "    elif isinstance(uploaded_file, bytes):\n",
    "        return uploaded_file\n",
    "    else:\n",
    "        # Try to print debug info\n",
    "        print(f\"File object type: {type(uploaded_file)}\")\n",
    "        print(f\"Available attributes: {dir(uploaded_file)}\")\n",
    "        raise ValueError(\"Could not extract image data from the uploaded file\")\n",
    "\n",
    "# Function to generate Grad-CAM visualization\n",
    "def generate_gradcam_visualization(image_path):\n",
    "    \"\"\"Generate Grad-CAM visualization for the given image path\"\"\"\n",
    "    # Load and preprocess the image\n",
    "    orig_image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(orig_image).to(config.DEVICE)\n",
    "    \n",
    "    # Get target layer for Grad-CAM\n",
    "    target_layer = get_gradcam_layer(model)\n",
    "    \n",
    "    # Generate Grad-CAM\n",
    "    raw_logit, heatmap, superimposed = generate_gradcam(model, image_tensor, orig_image)\n",
    "    \n",
    "    # Calculate the prediction\n",
    "    orig_prob = torch.sigmoid(torch.tensor(raw_logit)).item()\n",
    "    \n",
    "    # Apply calibration for fake images\n",
    "    if orig_prob < 0.5:  # Predicted as fake\n",
    "        calibrated_prob = 1.0 - (2.0 * orig_prob)\n",
    "    else:  # Predicted as real\n",
    "        calibrated_prob = orig_prob\n",
    "    \n",
    "    pred_class = \"Real\" if orig_prob >= 0.5 else \"Fake\"\n",
    "    \n",
    "    return orig_image, superimposed, pred_class, calibrated_prob\n",
    "\n",
    "# Function to process uploaded image and show result\n",
    "def process_image(uploaded_file, use_gradcam=False):\n",
    "    try:\n",
    "        # Extract image data\n",
    "        img_data = extract_image_data(uploaded_file)\n",
    "        \n",
    "        # Save to temp file (needed for predict_image_calibrated)\n",
    "        temp_path = \"temp_uploaded_image.jpg\"\n",
    "        with open(temp_path, \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "        \n",
    "        if use_gradcam:\n",
    "            # Generate Grad-CAM visualization\n",
    "            orig_image, superimposed, pred_class, prob = generate_gradcam_visualization(temp_path)\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "            fig.subplots_adjust(top=0.85)  # Make more room for title\n",
    "            \n",
    "            # Original image\n",
    "            ax1.imshow(orig_image)\n",
    "            ax1.set_title(\"Original Image\", fontsize=14, pad=10)\n",
    "            ax1.axis('off')\n",
    "            \n",
    "            # Grad-CAM visualization\n",
    "            ax2.imshow(superimposed)\n",
    "            \n",
    "            # Set title color based on prediction\n",
    "            title_color = 'green' if pred_class == 'Real' else 'red'\n",
    "            ax2.set_title(f\"Grad-CAM: {pred_class} ({prob:.4f})\", \n",
    "                        fontsize=14, color=title_color, pad=10)\n",
    "            ax2.axis('off')\n",
    "            \n",
    "            # Add information about prediction above the figures\n",
    "            plt.suptitle(f\"Prediction: {pred_class} ({prob:.4f})\", \n",
    "                       fontsize=16, y=0.98, color=title_color)\n",
    "            \n",
    "            # Add extra space between plots\n",
    "            plt.tight_layout(pad=3.0)\n",
    "        else:\n",
    "            # Use existing predict_image_calibrated from inference.py\n",
    "            prob, pred_class = predict_image_calibrated(model, temp_path, transform)\n",
    "            \n",
    "            # Create visualization\n",
    "            image = Image.open(temp_path).convert('RGB')\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            \n",
    "            color = 'green' if pred_class == 'Real' else 'red'\n",
    "            plt.title(f\"Prediction: {pred_class} (Confidence: {prob:.4f})\", \n",
    "                    color=color, fontsize=16)\n",
    "        \n",
    "        # Save to buffer for display\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Remove temp file\n",
    "        os.remove(temp_path)\n",
    "        \n",
    "        # Convert to base64 for display\n",
    "        img_str = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        \n",
    "        # Create HTML result\n",
    "        if use_gradcam:\n",
    "            result_html = f\"\"\"\n",
    "            <div style=\"text-align: center;\">\n",
    "                <h2>GANFingerprint Analysis Result (with Grad-CAM)</h2>\n",
    "                <img src=\"data:image/png;base64,{img_str}\" style=\"max-width: 800px\">\n",
    "                <h3 style=\"color: {title_color}\">Prediction: {pred_class}</h3>\n",
    "                <p>Confidence: {prob:.4f}</p>\n",
    "                <p><small>Grad-CAM highlights the regions that influenced the model's decision (red/yellow areas = more influence)</small></p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            result_html = f\"\"\"\n",
    "            <div style=\"text-align: center;\">\n",
    "                <h2>GANFingerprint Analysis Result</h2>\n",
    "                <img src=\"data:image/png;base64,{img_str}\" style=\"max-width: 500px\">\n",
    "                <h3 style=\"color: {'green' if pred_class == 'Real' else 'red'}\">Prediction: {pred_class}</h3>\n",
    "                <p>Confidence: {prob:.4f}</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        return result_html\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        # Clean up temp file if it exists\n",
    "        if 'temp_path' in locals() and os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        \n",
    "        # Return error message\n",
    "        return f\"\"\"\n",
    "        <div style=\"text-align: center; color: red; padding: 20px; background-color: #fff3f3; border-radius: 10px;\">\n",
    "            <h2>Error Processing Image</h2>\n",
    "            <p>{str(e)}</p>\n",
    "            <pre style=\"text-align: left; background-color: #f8f8f8; padding: 10px; max-height: 300px; overflow: auto;\">\n",
    "{traceback.format_exc()}\n",
    "            </pre>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "# Create the interface\n",
    "header_html = \"\"\"\n",
    "<div style=\"text-align: center; margin-bottom: 20px; background-color: #f0f0f0; padding: 20px; border-radius: 10px;\">\n",
    "    <h1 style=\"color: #333333;\">GANFingerprint Detector</h1>\n",
    "    <p style=\"color: #333333;\">Upload an image to determine if it's a real photo or AI-generated</p>\n",
    "    <p style=\"font-size: 0.8em; color: #666;\">For best results, use images that are 256x256 pixels</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(header_html))\n",
    "\n",
    "# Create upload widget\n",
    "upload = widgets.FileUpload(\n",
    "    accept='image/*',\n",
    "    multiple=False,\n",
    "    description='Select Image',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Create analyze button\n",
    "button = widgets.Button(\n",
    "    description='Analyze Image',\n",
    "    button_style='primary',\n",
    "    disabled=True,\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Create Grad-CAM toggle\n",
    "gradcam_toggle = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Show Grad-CAM Visualization',\n",
    "    indent=False,\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Create tooltip for Grad-CAM toggle\n",
    "gradcam_info = widgets.HTML(\n",
    "    value=\"\"\"\n",
    "    <div style=\"font-size: 0.85em; color: #666; margin-top: 5px;\">\n",
    "        Grad-CAM shows which parts of the image influenced the model's decision\n",
    "    </div>\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create output area\n",
    "output = widgets.Output()\n",
    "\n",
    "# Enable button when file is uploaded\n",
    "def on_upload_change(change):\n",
    "    button.disabled = len(upload.value) == 0\n",
    "    \n",
    "upload.observe(on_upload_change, names='value')\n",
    "\n",
    "# Handle button click\n",
    "def on_button_click(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        if len(upload.value) == 0:\n",
    "            print(\"Please upload an image first\")\n",
    "            return\n",
    "        \n",
    "        display(HTML(\"<p>Analyzing image...</p>\"))\n",
    "        \n",
    "        # Get the uploaded file\n",
    "        if isinstance(upload.value, dict):\n",
    "            # Older ipywidgets format\n",
    "            uploaded_file = list(upload.value.values())[0]\n",
    "        else:\n",
    "            # Newer ipywidgets format\n",
    "            uploaded_file = upload.value[0]\n",
    "        \n",
    "        # Process the image and display result\n",
    "        result_html = process_image(uploaded_file, use_gradcam=gradcam_toggle.value)\n",
    "        clear_output()\n",
    "        display(HTML(result_html))\n",
    "\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Layout the interface\n",
    "gradcam_controls = widgets.VBox([gradcam_toggle, gradcam_info], \n",
    "                              layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "controls = widgets.HBox([upload, button, gradcam_controls], \n",
    "                      layout=widgets.Layout(justify_content='center', \n",
    "                                          display='flex',\n",
    "                                          margin='20px 0'))\n",
    "display(controls)\n",
    "display(output)\n",
    "\n",
    "print(f\"Interface loaded. Using model from {MODEL_CHECKPOINT}\")\n",
    "print(\"Ready to analyze images!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
