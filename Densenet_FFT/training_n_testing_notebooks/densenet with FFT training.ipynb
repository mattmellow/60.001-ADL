{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2619d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Virtual Environment for model\n",
    "!python3 -m venv myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab18cc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-08T17:06:34.514106Z",
     "iopub.status.busy": "2025-04-08T17:06:34.513762Z",
     "iopub.status.idle": "2025-04-08T17:06:43.722397Z",
     "shell.execute_reply": "2025-04-08T17:06:43.721486Z"
    },
    "papermill": {
     "duration": 9.215549,
     "end_time": "2025-04-08T17:06:43.724198",
     "exception": false,
     "start_time": "2025-04-08T17:06:34.508649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02b66a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:06:43.732533Z",
     "iopub.status.busy": "2025-04-08T17:06:43.732101Z",
     "iopub.status.idle": "2025-04-08T17:06:44.233972Z",
     "shell.execute_reply": "2025-04-08T17:06:44.232929Z"
    },
    "papermill": {
     "duration": 0.507648,
     "end_time": "2025-04-08T17:06:44.235765",
     "exception": false,
     "start_time": "2025-04-08T17:06:43.728117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define dataset paths\n",
    "dataset_path = '../../data'\n",
    "train_path = os.path.join(dataset_path, \"train\")\n",
    "val_path = os.path.join(dataset_path, \"validation\")\n",
    "test_path = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d528794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:06:50.103221Z",
     "iopub.status.busy": "2025-04-08T17:06:50.102970Z",
     "iopub.status.idle": "2025-04-08T17:06:50.107515Z",
     "shell.execute_reply": "2025-04-08T17:06:50.106803Z"
    },
    "papermill": {
     "duration": 0.009825,
     "end_time": "2025-04-08T17:06:50.108853",
     "exception": false,
     "start_time": "2025-04-08T17:06:50.099028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply FFT to each image in the data pipeline\n",
    "def apply_fft(image):\n",
    "    image_tensor = transforms.ToTensor()(image).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "    fft_image = torch.fft.fftshift(torch.fft.fft2(image_tensor))  # Perform FFT and shift zero frequency to center\n",
    "    fft_image = torch.abs(fft_image)  # Take magnitude\n",
    "    fft_image = torch.log(fft_image + 1e-5)  # Avoid log(0) by adding a small constant\n",
    "    \n",
    "    # Ensure that the transformed image tensor requires gradients\n",
    "    fft_image.requires_grad = True\n",
    "    \n",
    "    return fft_image.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "# Custom transformation to apply FFT in the data pipeline\n",
    "class FFTTransform:\n",
    "    def __call__(self, image):\n",
    "        return apply_fft(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9131497",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:06:50.115879Z",
     "iopub.status.busy": "2025-04-08T17:06:50.115652Z",
     "iopub.status.idle": "2025-04-08T17:06:50.120239Z",
     "shell.execute_reply": "2025-04-08T17:06:50.119577Z"
    },
    "papermill": {
     "duration": 0.009553,
     "end_time": "2025-04-08T17:06:50.121462",
     "exception": false,
     "start_time": "2025-04-08T17:06:50.111909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define transforms for training and validation with FFT\n",
    "transformation_for_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    FFTTransform(),  # Apply FFT\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Directly normalize the tensor\n",
    "])\n",
    "\n",
    "transformation_for_valntest = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    FFTTransform(),  # Apply FFT\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Directly normalize the tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab29f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:06:50.128211Z",
     "iopub.status.busy": "2025-04-08T17:06:50.127979Z",
     "iopub.status.idle": "2025-04-08T17:10:39.520684Z",
     "shell.execute_reply": "2025-04-08T17:10:39.519658Z"
    },
    "papermill": {
     "duration": 229.398229,
     "end_time": "2025-04-08T17:10:39.522667",
     "exception": false,
     "start_time": "2025-04-08T17:06:50.124438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(train_path)\n",
    "train_dataset = datasets.ImageFolder(root=train_path, transform=transformation_for_train)\n",
    "val_dataset = datasets.ImageFolder(root=val_path, transform=transformation_for_valntest)\n",
    "test_dataset = datasets.ImageFolder(root=test_path, transform=transformation_for_valntest)\n",
    "\n",
    "# DataLoader with batch size\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aea466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:10:39.531210Z",
     "iopub.status.busy": "2025-04-08T17:10:39.530915Z",
     "iopub.status.idle": "2025-04-08T17:10:39.539121Z",
     "shell.execute_reply": "2025-04-08T17:10:39.538424Z"
    },
    "papermill": {
     "duration": 0.013862,
     "end_time": "2025-04-08T17:10:39.540508",
     "exception": false,
     "start_time": "2025-04-08T17:10:39.526646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate recall and accuracy\n",
    "def compute_metrics(outputs, labels):\n",
    "    # Convert the logits to binary predictions\n",
    "    predicted = (torch.sigmoid(outputs) > 0.5).float()  # Predictions as 0 or 1\n",
    "    \n",
    "    # True positives, false positives, false negatives, true negatives\n",
    "    tp = torch.sum((predicted == 1) & (labels == 1)).item()  # True positives\n",
    "    fp = torch.sum((predicted == 1) & (labels == 0)).item()  # False positives\n",
    "    fn = torch.sum((predicted == 0) & (labels == 1)).item()  # False negatives\n",
    "    tn = torch.sum((predicted == 0) & (labels == 0)).item()  # True negatives\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0  # Avoid division by zero\n",
    "    \n",
    "    # Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Avoid division by zero\n",
    "    \n",
    "    # F1-Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0  # Avoid division by zero\n",
    "    \n",
    "    return accuracy, recall, precision, f1\n",
    "\n",
    "\n",
    "# Freezing and unfreezing model layers\n",
    "def freeze_everything_except_classifier(model):\n",
    "    # Get the parameters of the classifier for comparison\n",
    "    classifier_params = set(model.classifier.parameters())\n",
    "    \n",
    "    # Freeze all parameters except the classifier\n",
    "    for param in model.parameters():\n",
    "        if param not in classifier_params:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    print(\"Only training classifier\")\n",
    "\n",
    "\n",
    "def unfreeze_last_block(model):\n",
    "    for name, params in model.named_parameters():\n",
    "        if \"layer4\" in name or \"fc\" in name:\n",
    "            params.requires_grad = True\n",
    "        else:\n",
    "            params.requires_grad = False\n",
    "    print(\"Training last block and classifier\")\n",
    "\n",
    "\n",
    "def unfreeze_last_two_blocks(model):\n",
    "    for name, params in model.named_parameters():\n",
    "        if \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n",
    "            params.requires_grad = True\n",
    "        else:\n",
    "            params.requires_grad = False\n",
    "    print(\"Training last 2 blocks and classifier\")\n",
    "\n",
    "\n",
    "def unfreeze_whole_model(model):\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad = True\n",
    "    print(\"Whole model training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca1c9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:10:40.735184Z",
     "iopub.status.busy": "2025-04-08T17:10:40.734918Z",
     "iopub.status.idle": "2025-04-08T17:10:41.648464Z",
     "shell.execute_reply": "2025-04-08T17:10:41.647454Z"
    },
    "papermill": {
     "duration": 0.919051,
     "end_time": "2025-04-08T17:10:41.650140",
     "exception": false,
     "start_time": "2025-04-08T17:10:40.731089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "densenetmodel= models.densenet121(pretrained=True)\n",
    "no_features= densenetmodel.classifier.in_features\n",
    "densenetmodel.classifier = nn.Linear(no_features,1)\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "densenetmodel = densenetmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfa7e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:10:41.658915Z",
     "iopub.status.busy": "2025-04-08T17:10:41.658647Z",
     "iopub.status.idle": "2025-04-08T17:10:41.662292Z",
     "shell.execute_reply": "2025-04-08T17:10:41.661617Z"
    },
    "papermill": {
     "duration": 0.009622,
     "end_time": "2025-04-08T17:10:41.663666",
     "exception": false,
     "start_time": "2025-04-08T17:10:41.654044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "phases = [\n",
    "    {\"epochs\": 10, \"unfreeze\": freeze_everything_except_classifier, \"lr\": 1e-3},\n",
    "    {\"epochs\": 10, \"unfreeze\": unfreeze_last_block, \"lr\": 1e-4},\n",
    "    {\"epochs\": 10, \"unfreeze\": unfreeze_last_two_blocks, \"lr\": 1e-5},\n",
    "    {\"epochs\": 10, \"unfreeze\": unfreeze_whole_model, \"lr\": 1e-6}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f087820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:10:41.671968Z",
     "iopub.status.busy": "2025-04-08T17:10:41.671748Z",
     "iopub.status.idle": "2025-04-08T23:10:10.273353Z",
     "shell.execute_reply": "2025-04-08T23:10:10.272635Z"
    },
    "papermill": {
     "duration": 21568.607608,
     "end_time": "2025-04-08T23:10:10.275122",
     "exception": false,
     "start_time": "2025-04-08T17:10:41.667514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "with open(\"training_log.txt\", \"w\") as log_file:\n",
    "    for phase_idx, phase in enumerate(phases):\n",
    "        densenetmodel= models.densenet121(pretrained=True)\n",
    "        no_features= densenetmodel.classifier.in_features\n",
    "        densenetmodel.classifier = nn.Linear(no_features,1)\n",
    "        device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        densenetmodel = densenetmodel.to(device)\n",
    "        densenetmodel.train()\n",
    "        \n",
    "        phase[\"unfreeze\"](densenetmodel)\n",
    "        optimiser = optim.Adam(densenetmodel.parameters(), lr=phase[\"lr\"])\n",
    "        log_file.write(f\"Starting phase {phase_idx + 1}: {phase['unfreeze'].__name__} | Learning Rate: {phase['lr']}\\n\")\n",
    "\n",
    "        for epoch in range(phase[\"epochs\"]):\n",
    "            densenetmodel.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "\n",
    "                optimiser.zero_grad()\n",
    "                outputs = densenetmodel(inputs).squeeze()  # Get model output\n",
    "                loss = criterion(outputs, labels)  # Compute loss\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                accuracy, recall, precision, f1 = compute_metrics(outputs, labels)\n",
    "                \n",
    "                # Track metrics\n",
    "                correct += accuracy\n",
    "                total += 1\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            epoch_accuracy = correct / total\n",
    "            log_file.write(f\"Phase {phase_idx + 1}: {phase['unfreeze'].__name__}, Epoch [{epoch + 1}/{phase['epochs']}], \"\n",
    "                           f\"Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\\n\")\n",
    "\n",
    "            print(f\"Phase {phase_idx + 1}: Epoch [{epoch + 1}/{phase['epochs']}], Loss: {epoch_loss:.4f}, \"\n",
    "                  f\"Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "            checkpoint_filename = f\"../models/fftdensenet_phase{phase_idx+1}epoch_{epoch + 1}.pth\"\n",
    "            torch.save(densenetmodel.state_dict(), checkpoint_filename)\n",
    "            print(f\"Model saved as fftdensenet_phase{phase_idx+1}epoch_{epoch + 1}.pth\")\n",
    "\n",
    "            # Validation phase\n",
    "            densenetmodel.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "            true_positive = 0\n",
    "            false_positive = 0\n",
    "            false_negative = 0\n",
    "            true_negative = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.float().unsqueeze(1).to(device)\n",
    "                    # Get model predictions\n",
    "                    outputs = densenetmodel(inputs).squeeze()  # Get model output\n",
    "                    # Ensure the labels have the same shape as the model output\n",
    "                    labels = labels.view(-1)\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                    # Compute metrics manually\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert to binary predictions\n",
    "\n",
    "\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Update counts for recall and precision\n",
    "                    true_positive += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "                    false_positive += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "                    false_negative += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "                    true_negative += ((predicted == 0) & (labels == 0)).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "            correct = true_positive+true_negative\n",
    "\n",
    "            # Compute metrics manually\n",
    "            avg_loss = running_loss / len(test_loader)\n",
    "            avg_accuracy = correct / total\n",
    "\n",
    "            # Recall = true_positive / (true_positive + false_negative)\n",
    "            avg_recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "\n",
    "            # Precision = true_positive / (true_positive + false_positive)\n",
    "            avg_precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "\n",
    "            # F1 Score = 2 * (precision * recall) / (precision + recall)\n",
    "            avg_f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "\n",
    "\n",
    "            # Write test results to the log file\n",
    "            log_file.write(f\"Validation Loss: {avg_loss:.4f}\\n\")\n",
    "            log_file.write(f\"Validation Accuracy: {avg_accuracy:.4f}\\n\")\n",
    "            log_file.write(f\"Validation Precision: {avg_precision:.4f}\\n\")\n",
    "            log_file.write(f\"Validation Recall: {avg_recall:.4f}\\n\")\n",
    "            log_file.write(f\"Validation F1 Score: {avg_f1:.4f}\\n\")\n",
    "            log_file.write(\"=\" * 50 + \"\\n\")  # Separator for clarity"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1909705,
     "sourceId": 3134515,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 291335,
     "modelInstanceId": 270348,
     "sourceId": 320548,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21892.425943,
   "end_time": "2025-04-08T23:11:23.959997",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-08T17:06:31.534054",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
