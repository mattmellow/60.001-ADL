{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-25T16:24:02.726956Z",
     "iopub.status.busy": "2025-03-25T16:24:02.726593Z",
     "iopub.status.idle": "2025-03-25T16:24:02.731970Z",
     "shell.execute_reply": "2025-03-25T16:24:02.730892Z",
     "shell.execute_reply.started": "2025-03-25T16:24:02.726932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:24:02.756296Z",
     "iopub.status.busy": "2025-03-25T16:24:02.756065Z",
     "iopub.status.idle": "2025-03-25T16:24:02.774266Z",
     "shell.execute_reply": "2025-03-25T16:24:02.773413Z",
     "shell.execute_reply.started": "2025-03-25T16:24:02.756276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define dataset paths\n",
    "dataset_path = '../../'\n",
    "train_path = os.path.join(dataset_path, \"Dataset/Train\")\n",
    "val_path = os.path.join(dataset_path, \"Dataset/Validation\")\n",
    "test_path = os.path.join(dataset_path, \"Dataset/Test\")\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:24:02.776080Z",
     "iopub.status.busy": "2025-03-25T16:24:02.775815Z",
     "iopub.status.idle": "2025-03-25T16:24:02.793237Z",
     "shell.execute_reply": "2025-03-25T16:24:02.792230Z",
     "shell.execute_reply.started": "2025-03-25T16:24:02.776058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Apply FFT to each image in the data pipeline\n",
    "def apply_fft(image):\n",
    "    image_tensor = transforms.ToTensor()(image).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "    fft_image = torch.fft.fftshift(torch.fft.fft2(image_tensor))  # Perform FFT and shift zero frequency to center\n",
    "    fft_image = torch.abs(fft_image)  # Take magnitude\n",
    "    fft_image = torch.log(fft_image + 1e-5)  # Avoid log(0) by adding a small constant\n",
    "    \n",
    "    # Ensure that the transformed image tensor requires gradients\n",
    "    fft_image.requires_grad = True\n",
    "    \n",
    "    return fft_image.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "# Custom transformation to apply FFT in the data pipeline\n",
    "class FFTTransform:\n",
    "    def __call__(self, image):\n",
    "        return apply_fft(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for training and validation with FFT\n",
    "transformation_for_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    FFTTransform(),  # Apply FFT\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Directly normalize the tensor\n",
    "])\n",
    "\n",
    "transformation_for_valntest = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    FFTTransform(),  # Apply FFT\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Directly normalize the tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate recall and accuracy\n",
    "def compute_metrics(outputs, labels):\n",
    "    # Convert the logits to binary predictions\n",
    "    predicted = (torch.sigmoid(outputs) > 0.5).float()  # Predictions as 0 or 1\n",
    "    \n",
    "    # True positives, false positives, false negatives, true negatives\n",
    "    tp = torch.sum((predicted == 1) & (labels == 1)).item()  # True positives\n",
    "    fp = torch.sum((predicted == 1) & (labels == 0)).item()  # False positives\n",
    "    fn = torch.sum((predicted == 0) & (labels == 1)).item()  # False negatives\n",
    "    tn = torch.sum((predicted == 0) & (labels == 0)).item()  # True negatives\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Precision\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0  # Avoid division by zero\n",
    "    \n",
    "    # Recall\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0  # Avoid division by zero\n",
    "    \n",
    "    # F1-Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0  # Avoid division by zero\n",
    "    \n",
    "    return accuracy, recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-25T16:24:02.827194Z",
     "iopub.status.busy": "2025-03-25T16:24:02.826942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(train_path)\n",
    "val_dataset = datasets.ImageFolder(root=val_path, transform=transformation_for_valntest)\n",
    "test_dataset = datasets.ImageFolder(root=test_path, transform=transformation_for_valntest)\n",
    "\n",
    "# DataLoader with batch size\n",
    "batch_size = 32\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory containing the model folders\n",
    "\n",
    "root_path = \"../models/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "densenetmodel = models.densenet121(pretrained=False)\n",
    "no_features = densenetmodel.classifier.in_features\n",
    "densenetmodel.classifier = nn.Linear(no_features, 1)\n",
    "model = densenetmodel.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Iterate over the folders in the root directory\n",
    "for saved_model in os.listdir(root_path):\n",
    "    densenetmodel = models.densenet121(pretrained=False)\n",
    "    no_features = densenetmodel.classifier.in_features\n",
    "    densenetmodel.classifier = nn.Linear(no_features, 1)\n",
    "    model = densenetmodel.to(device)\n",
    "    model.eval()\n",
    "    evaluation_metrics=[]\n",
    "    # Check if the file ends with .pth\n",
    "    if saved_model.endswith(\".pth\"):\n",
    "        # Define the path for the pickle file\n",
    "        pickle_filename = os.path.splitext(saved_model)[0] + \"_evaluation.pkl\"\n",
    "        pickle_file_path = os.path.join(root_path, pickle_filename)\n",
    "        \n",
    "        # Check if the pickle file already exists\n",
    "        if os.path.exists(pickle_file_path):\n",
    "            print(f\"Loading the pickle file: {pickle_file_path}\")\n",
    "            with open(pickle_file_path, 'rb') as f:\n",
    "                evaluation_metrics = pickle.load(f)\n",
    "\n",
    "        else:\n",
    "            print(f\"Loading the file: {saved_model}\")\n",
    "            model_path=os.path.join(root_path,saved_model)\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            model.eval()\n",
    "            model = model.to(device)\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            # Testing loop\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "            true_positive = 0\n",
    "            false_positive = 0\n",
    "            false_negative = 0\n",
    "            true_negative = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.float().unsqueeze(1).to(device)\n",
    "                    # Get model predictions\n",
    "                    outputs = model(inputs).squeeze()  # Get model output\n",
    "                    labels = labels.view(-1)\n",
    "    \n",
    "                    # Compute loss\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    running_loss += loss.item()\n",
    "                    # Compute metrics manually\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert to binary predictions\n",
    "                    total += labels.size(0)\n",
    "                    # Update counts for recall and precision\n",
    "                    true_positive += ((predicted == 1) & (labels == 1)).sum().item()\n",
    "                    false_positive += ((predicted == 1) & (labels == 0)).sum().item()\n",
    "                    false_negative += ((predicted == 0) & (labels == 1)).sum().item()\n",
    "                    true_negative += ((predicted == 0) & (labels == 0)).sum().item()\n",
    "                    all_preds.extend(predicted.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "            correct = true_positive + true_negative\n",
    "            # Compute metrics manually\n",
    "            avg_accuracy = correct / total\n",
    "            avg_recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "            avg_precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "            avg_f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "            # Collect the evaluation metrics for this epoch\n",
    "            epoch_metrics = {\n",
    "                \"Loss\": running_loss,\n",
    "                \"Accuracy\": avg_accuracy,\n",
    "                \"Precision\": avg_precision,\n",
    "                \"Recall\": avg_recall,\n",
    "                \"F1 Score\": avg_f1,\n",
    "                \"True Positive\": true_positive,\n",
    "                \"True Negative\": true_negative,\n",
    "                \"False Positive\": false_positive,\n",
    "                \"False Negative\": false_negative\n",
    "            }\n",
    "            evaluation_metrics.append(epoch_metrics)\n",
    "            \n",
    "            # Save evaluation metrics as a pickle file\n",
    "            with open(pickle_file_path, 'wb') as f:\n",
    "                pickle.dump(evaluation_metrics, f)\n",
    "            print(f\"Saved evaluation results to: {pickle_file_path}\")\n",
    "    \n",
    "        # After processing all weight files (epochs), create a DataFrame\n",
    "        df = pd.DataFrame(evaluation_metrics)\n",
    "        # Save the DataFrame to an Excel file (one per folder)\n",
    "        if not os.path.exists(\"../excel folder/\"):\n",
    "            os.makedirs(\"../excel folder/\")\n",
    "        excel_file=f\"../excel folder/{saved_model}_evaluation.xlsx\"\n",
    "        df.to_excel(excel_file, index=False)\n",
    "        print(f\"Saved evaluation results to: {excel_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1909705,
     "sourceId": 3134515,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
